{
  "hash": "5c569e62b35df676899070d482d4aba8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: '**Exploratory Data Analysis**'\n---\n\n\n---\n\n- Goal: understand distributions, class balance, and potential text‑based features that will inform feature‑engineering and model selection for M4.\n\n## Notebook Outline \n1. [Data & Imports](#1-data--imports)  \n2. [Class Balance](#2-class-balance)  \n3. [Tweet-Length Distributions](#3-tweet-length-distributions)  \n4. [Negative-Reason Breakdown](#4-negative-reason-breakdown)  \n5. [Sentiment by Airline](#5-sentiment-by-airline)  \n6. [Quick TF-IDF Peek — Top Terms by Sentiment](#6-quick-tf-idf-peek--top-terms-by-sentiment)  \n7. [Key Insights](#7-key-insights)\n\n\n# 1. Data & Imports  \n***\nLoad the pre‑processed dataset **`data/processed/tweets.parquet`** and perform a quick shape / null check.\n\n* **14 640 rows × 5 columns** after preprocessing — matches raw count (no data loss).  \n* Columns: `tweet_id`, `airline`, `airline_sentiment`, `clean_text`, `negativereason`  \n* *No unexpected nulls* (aside from `negativereason`, which is naturally sparse).\n\n::: {#fbb54f4f .cell execution_count=1}\n``` {.python .cell-code}\n# Imports ─────────────────────────────────────────────────\nfrom pathlib import Path\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# -------------------------------------------------------------------\n# repo-aware paths \n# -------------------------------------------------------------------\nPROJ_ROOT  = Path.cwd().parent                 # one level up from notebooks/\nFIG_DIR    = PROJ_ROOT / \"reports\" / \"figs_eda\"\nDATA_PATH  = PROJ_ROOT / \"data\" / \"processed\" / \"tweets.parquet\"\n\nFIG_DIR.mkdir(parents=True, exist_ok=True)\n\n# -------------------------------------------------------------------\n# load data and quick peek\n# -------------------------------------------------------------------\ndf = pd.read_parquet(DATA_PATH)\n\nprint(f\"Loaded {len(df):,} rows × {df.shape[1]} columns\")\ndisplay(df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoaded 14,640 rows × 5 columns\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>airline</th>\n      <th>airline_sentiment</th>\n      <th>clean_text</th>\n      <th>negativereason</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>570306133677760513</td>\n      <td>Virgin America</td>\n      <td>neutral</td>\n      <td>what  said.</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>570301130888122368</td>\n      <td>Virgin America</td>\n      <td>positive</td>\n      <td>plus you've added commercials to the experienc...</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570301083672813571</td>\n      <td>Virgin America</td>\n      <td>neutral</td>\n      <td>i didn't today... must mean i need to take ano...</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>570301031407624196</td>\n      <td>Virgin America</td>\n      <td>negative</td>\n      <td>it's really aggressive to blast obnoxious \"ent...</td>\n      <td>Bad Flight</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570300817074462722</td>\n      <td>Virgin America</td>\n      <td>negative</td>\n      <td>and it's a really big bad thing about it</td>\n      <td>Can't Tell</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# 2. Class Balance  \n***\nThe dataset is **strongly imbalanced** – negative tweets dominate.\n\n| Sentiment | Share |\n|-----------|-------|\n| **Negative** | **63 %** |\n| Neutral     | 21 % |\n| Positive    | 16 % |\n\n* **Implication →** apply stratified sampling and class weighting / resampling for any supervised model.\n\n::: {#b84b3a43 .cell execution_count=2}\n``` {.python .cell-code}\nsent_dist = df[\"airline_sentiment\"].value_counts(normalize=True)\nsns.barplot(x=sent_dist.index, y=sent_dist.values)\nplt.title(\"Class balance (share)\")\nplt.ylabel(\"Proportion\")\nplt.tight_layout()\nplt.savefig(\"../reports/figs_eda/class_balance.png\", dpi=150)\n```\n\n::: {.cell-output .cell-output-display}\n![](03_eda_sentiment_features_files/figure-html/cell-3-output-1.png){width=661 height=468}\n:::\n:::\n\n\n# 3. Tweet‑Length Distributions  \n***\nTwo views of tweet length after cleaning.\n\n| Metric | Median | IQR | 95th pct |\n|--------|--------|-----|-----------|\n| Characters | **108** | 74 – 129 | 167 |\n| Words      | **17**  | 11 – 22  | 29 |\n\n* Character distribution is right‑skewed but bounded < 180; word counts top out near 35.  \n* **Padding/truncation guideline →** 160 chars *or* 30 words captures > 99 % of tweets.\n\n::: {#a7f4b6c3 .cell execution_count=3}\n``` {.python .cell-code}\ndf[\"char_len\"]  = df[\"clean_text\"].str.len()\ndf[\"word_count\"] = df[\"clean_text\"].str.split().str.len()\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\nsns.histplot(df[\"char_len\"], ax=axes[0], bins=40)\naxes[0].set_title(\"Tweet length (char)\")\nsns.histplot(df[\"word_count\"], ax=axes[1], bins=40)\naxes[1].set_title(\"Tweet length (words)\")\nplt.tight_layout()\nplt.savefig(\"../reports/figs_eda/length_distributions.png\", dpi=150)\n```\n\n::: {.cell-output .cell-output-display}\n![](03_eda_sentiment_features_files/figure-html/cell-4-output-1.png){width=949 height=276}\n:::\n:::\n\n\n# 4. Negative‑Reason Breakdown \n***\nTop complaint categories among **negative** tweets.\n\n| Rank | Reason                     | Count | Share |\n|------|----------------------------|-------|-------|\n| 1 | Customer Service Issue        | ~3 000 | 33 % |\n| 2 | Late Flight                  | ~1 600 | 18 % |\n| 3 | Can’t Tell / N/A              | ~1 300 | 13 % |\n| 4 | Cancelled Flight             | ~1 000 | 11 % |\n| 5 | Lost Luggage                 |   800 |  9 % |\n\n* **Customer‑service** complaints dominate—consider a dedicated topic model or sentiment sub‑label.  \n* Tail reasons (< 3 %) can be merged or dropped to prevent feature sparsity.\n\n::: {#c0673119 .cell execution_count=4}\n``` {.python .cell-code}\nneg_counts = (df[df[\"airline_sentiment\"] == \"negative\"]\n              .value_counts(\"negativereason\")\n              .sort_values())\nneg_counts.plot(kind=\"barh\", figsize=(6,4))\nplt.title(\"Negative reasons\")\nplt.tight_layout()\nplt.savefig(\"../reports/figs_eda/negative_reasons.png\", dpi=150)\n```\n\n::: {.cell-output .cell-output-display}\n![](03_eda_sentiment_features_files/figure-html/cell-5-output-1.png){width=567 height=373}\n:::\n:::\n\n\n# 5. Sentiment by Airline  \n***\nTwo complementary views:\n\n* **Stacked 100 % bar** → distribution of sentiments per airline.  \n* **Diverging bar** → **Positive % − Negative %** (“net gap”).\n\n| Airline | Net gap | Insight |\n|---------|---------|---------|\n| US Airways | **‑0.66** | Most negative overall |\n| American   | ‑0.55     | Similar to US Airways |\n| United     | ‑0.50     | High negative volume |\n| Southwest  | ‑0.23     | Moderately better perception |\n| Delta      | ‑0.17     | Best among majors |\n| Virgin America | **‑0.03** | Nearly sentiment‑neutral |\n\n* **Virgin America** enjoys the **highest positive share (~ 34 %)**.  \n* Consider adding `airline` as a categorical feature or training airline‑specific models.\n\n::: {#cf8ff409 .cell execution_count=5}\n``` {.python .cell-code}\n# ── prepare data ─────────────────────────────────────────────────────\nsent_prop = (\n    df.pivot_table(index=\"airline\",\n                   columns=\"airline_sentiment\",\n                   values=\"tweet_id\",\n                   aggfunc=\"count\",\n                   fill_value=0)\n      .pipe(lambda t: t.div(t.sum(axis=1), axis=0))               # row %\n      [[\"negative\", \"neutral\", \"positive\"]]                       # order\n)\ngap = (sent_prop[\"positive\"] - sent_prop[\"negative\"]).sort_values()\n\n# ── plot combined figure ─────────────────────────────────────────────\npalette = [\"#d62728\", \"#ffbf00\", \"#2ca02c\"]\n\nfig, axes = plt.subplots(\n    1, 2, figsize=(10, 3), gridspec_kw={\"wspace\": 0.35}, constrained_layout=True\n)\n\n# left: stacked 100 %\nsent_prop.plot(kind=\"bar\", stacked=True, width=0.8,\n               color=palette, ax=axes[0])\naxes[0].set_ylabel(\"Proportion\")\naxes[0].set_xlabel(\"\")\naxes[0].set_title(\"Sentiment share by airline\")\naxes[0].legend(title=\"\", loc=\"upper left\", bbox_to_anchor=(1.04, 1))\n\n# right: diverging bar\nsns.barplot(x=gap.values, y=gap.index, ax=axes[1])\nfor bar, v in zip(axes[1].patches, gap.values):\n    bar.set_facecolor(\"#d62728\" if v < 0 else \"#2ca02c\")\naxes[1].set_xlabel(\"Positive % – Negative %\")\naxes[1].set_ylabel(\"\")\naxes[1].axvline(0, color=\"k\", lw=0.8)\naxes[1].set_title(\"Net sentiment gap\")\n\n# ── DISPLAY inline before opening any new figures ───────────────────\ndisplay(fig)\nplt.close(fig) \n\n# ── SAVE files (after display) ──────────────────────────────────────\nfig.savefig(FIG_DIR / \"sentiment_share_and_gap.png\", dpi=150)\n\n# individual images\nfor name, f in [(\"sentiment_share_by_airline.png\", sent_prop),\n                (\"net_sentiment_gap.png\", gap)]:\n    plt.figure(figsize=(4,3))\n    if name.startswith(\"sentiment\"):\n        f.plot(kind=\"bar\", stacked=True, width=0.8, color=palette, legend=False)\n    else:\n        sns.barplot(x=f.values, y=f.index)\n        for bar, v in zip(plt.gca().patches, f.values):\n            bar.set_facecolor(\"#d62728\" if v < 0 else \"#2ca02c\")\n        plt.axvline(0, color=\"k\", lw=0.8)\n    plt.tight_layout()\n    plt.savefig(FIG_DIR / name, dpi=150)\n    plt.close()\n# %% ------------------------------------------------------------------\n```\n\n::: {.cell-output .cell-output-display}\n![](03_eda_sentiment_features_files/figure-html/cell-6-output-1.png){width=971 height=299}\n:::\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 384x288 with 0 Axes>\n```\n:::\n:::\n\n\n# 6. Quick TF‑IDF Peek — Top Terms by Sentiment  \n***\n\n| Negative | Neutral | Positive |\n|----------|---------|----------|\n| flight | flight | thanks |\n| cancelled | thanks | thank |\n| service | dm | flight |\n| hours | fleek | great |\n| customer | fleet | love |\n\n* Negative tweets emphasise disruption words **“cancelled”, “service”, “hours”**.  \n* Positive tweets dominated by gratitude **“thanks”, “love”, “great”**.  \n* Confirms that the cleaning pipeline retained sentiment‑bearing tokens and removed noise.\n\n::: {#4c327422 .cell execution_count=6}\n``` {.python .cell-code}\n# ── build separate corpora per sentiment ────────────────────────────────\ncorpora = {\n    sentiment: df.loc[df[\"airline_sentiment\"] == sentiment, \"clean_text\"].tolist()\n    for sentiment in [\"negative\", \"neutral\", \"positive\"]\n}\n\n# ── fit TF-IDF and grab top 10 terms for each ────────────────────────────\nvectorizer = TfidfVectorizer(\n    stop_words=\"english\",\n    max_features=5_000,\n    ngram_range=(1, 2),\n)\n\ntop_terms = {}\nfor sentiment, texts in corpora.items():\n    tfidf_matrix = vectorizer.fit_transform(texts)\n    # average tf-idf per term & get top 10 indices\n    avg_scores = tfidf_matrix.mean(axis=0).A1\n    top_idx = avg_scores.argsort()[::-1][:10]\n    top_terms[sentiment] = [vectorizer.get_feature_names_out()[i] for i in top_idx]\n\n# ── show as a DataFrame ─────────────────────────────────────────────────\ntop_df = pd.DataFrame(top_terms)\ndisplay(top_df)\n\n# ── save to CSV ──────────────────────────────────────────────\ntop_df.to_csv(FIG_DIR / \"top_tfidf_terms.csv\", index=False)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>negative</th>\n      <th>neutral</th>\n      <th>positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>flight</td>\n      <td>flight</td>\n      <td>thanks</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cancelled</td>\n      <td>thanks</td>\n      <td>thank</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>service</td>\n      <td>dm</td>\n      <td>flight</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>hours</td>\n      <td>fleek</td>\n      <td>great</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>customer</td>\n      <td>fleet</td>\n      <td>just</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>just</td>\n      <td>fleet fleek</td>\n      <td>love</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>hold</td>\n      <td>just</td>\n      <td>service</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>help</td>\n      <td>flights</td>\n      <td>best</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>time</td>\n      <td>help</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>plane</td>\n      <td>need</td>\n      <td>awesome</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# 7. Key Insights  \n***\n* **Label imbalance** (63 % negative) demands stratified splits and weighting.  \n* **Typical tweet length** ≈ 110 chars / 17 words → standard tokenizer window is sufficient.  \n* **Customer‑service** and **late‑flight** issues form > 50 % of negative reasons.  \n* **Virgin America** and **Southwest** exhibit more positive sentiment → airline feature likely informative.  \n* TF‑IDF top terms validate domain‑specific vocabulary that will aid feature engineering.\n\n## Next Steps  \n1. Finalise feature list — text (TF‑IDF / embeddings) + metadata (`airline`, time, etc.).  \n2. Select resampling or class‑weight strategy for modelling.  \n3. Prototype baseline models in **`04_baseline_model.ipynb`** (log‑reg, SVM, BERT).  \n4. Feed these findings into the project README and roadmap.\n\n",
    "supporting": [
      "03_eda_sentiment_features_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}