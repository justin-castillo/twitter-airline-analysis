{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d51277",
   "metadata": {},
   "source": [
    "# **DistilBERT Transformer**\n",
    "***\n",
    "Fine‑tune a lightweight transformer (DistilBERT) on the **Twitter‑Airline Sentiment** dataset and benchmark it against a classical **TF‑IDF + Logistic Regression** baseline.\n",
    "\n",
    "> **Model** `distilbert‑base‑uncased`  \n",
    "> **Training split** 90 % of cleaned data (stratified)  \n",
    "> **Validation split** 10 % (held‑out during fine‑tuning)  \n",
    "> **Test set** Untouched split created in `04_baseline_model.ipynb`  \n",
    "> **Artifacts saved to** `models/distilbert_twitter/`\n",
    "\n",
    "## Notebook Outline\n",
    "1. [Imports & Global Config](#1-imports--global-config)  \n",
    "2. [Load Pre-Split Feather Data](#2-load-pre-split-feather-data)  \n",
    "3. [Tokenisation → HF `Dataset` Objects](#3-tokenisation--hf-dataset-objects)  \n",
    "4. [Model Instantiation](#4-model-instantiation)  \n",
    "5. [Training Configuration](#5-training-configuration)  \n",
    "6. [Trainer & Fine-Tune Results](#6-trainer--fine-tune-results)  \n",
    "7. [Save Artifacts & Export](#7-save-artifacts--export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84afaf23",
   "metadata": {},
   "source": [
    "# 1. Imports & Global Config\n",
    "***\n",
    "\n",
    "Everything we need in one place:\n",
    "\n",
    "1. **Path handling** (`pathlib.Path`) so the notebook is platform‑agnostic.  \n",
    "2. **Reproducibility seeds** for Python, NumPy, and (if available) CUDA.  \n",
    "3. **Key Hugging Face classes** (`AutoTokenizer`, `AutoModelForSequenceClassification`, `Trainer`, …).  \n",
    "4. A line that tells Transformers to **ignore TensorFlow** so only PyTorch is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac48629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"          # use PyTorch only\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# reproducibility \n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# repo‑aware paths \n",
    "PROJ_ROOT = Path.cwd().parent\n",
    "PROC_DIR  = PROJ_ROOT / \"data\" / \"processed\"\n",
    "MODEL_DIR = PROJ_ROOT / \"models\" / \"distilbert_twitter\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfdf44",
   "metadata": {},
   "source": [
    "# 2. Load Pre-Split Feather Data  \n",
    "***\n",
    "\n",
    "| Split | Rows | Columns |\n",
    "|-------|------|---------|\n",
    "| **train** | **11 712** | `text` (tweet) |\n",
    "| **val**   | **1 464**  | `text` (tweet) |\n",
    "\n",
    "*Assertion checks* ensure that every tweet is paired with exactly one sentiment label.\n",
    "\n",
    "> **Take-away:** the data already passed cleaning and stratified splitting elsewhere in the pipeline—nothing to redo here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | rows: 11,712\n",
      "val   | rows: 1,464\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>over an hour on hold so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>your gif game is strong.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm excited too, but perhaps you could scale y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>while other airlines weren't cancelled flighti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conf number fmjtyl delayed - any chance of get...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                        over an hour on hold so far\n",
       "1                           your gif game is strong.\n",
       "2  i'm excited too, but perhaps you could scale y...\n",
       "3  while other airlines weren't cancelled flighti...\n",
       "4  conf number fmjtyl delayed - any chance of get..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    negative\n",
       "1    negative\n",
       "2    positive\n",
       "3    negative\n",
       "4     neutral\n",
       "Name: label, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre‑made Feather splits \n",
    "def _load_xy_split(split: str):\n",
    "    \"\"\"\n",
    "    Return (X, y) for the given split.\n",
    "    X : DataFrame with 'text'\n",
    "    y : Series with 'label'\n",
    "    \"\"\"\n",
    "    X = pd.read_feather(PROC_DIR / f\"X_{split}.ftr\")        # ['text']\n",
    "    y = pd.read_feather(PROC_DIR / f\"y_{split}.ftr\")[\"label\"]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = _load_xy_split(\"train\")\n",
    "X_val,   y_val   = _load_xy_split(\"val\")\n",
    "\n",
    "for name, X, y in [(\"train\", X_train, y_train), (\"val\", X_val, y_val)]:\n",
    "    assert list(X.columns) == [\"text\"]\n",
    "    assert y.name == \"label\"\n",
    "    assert len(X) == len(y)\n",
    "    print(f\"{name:5} | rows: {len(X):,}\")\n",
    "\n",
    "display(X_train.head())\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf81ad",
   "metadata": {},
   "source": [
    "# 3. Tokenisation → HF `Dataset` Objects  \n",
    "***\n",
    "\n",
    "1. **Label mapping**  \n",
    "   `label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}`  \n",
    "2. **Tokenizer** — `distilbert-base-uncased` converts each tweet into `input_ids` & `attention_mask` (max len 128).  \n",
    "3. **Conversion** — `Dataset.from_pandas` yields memory-mapped datasets; raw text columns are dropped.\n",
    "\n",
    "| Dataset | Columns retained | Rows |\n",
    "|---------|------------------|------|\n",
    "| **train_ds** | `input_ids`, `attention_mask`, `labels` | 11 712 |\n",
    "| **val_ds**   | `input_ids`, `attention_mask`, `labels` | 1 464  |\n",
    "\n",
    "> **Why HF Datasets?** Zero-copy slices during training and built-in compatibility with `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89e742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11712/11712 [00:01<00:00, 10540.41 examples/s]\n",
      "Map: 100%|██████████| 1464/1464 [00:00<00:00, 3819.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds → ['input_ids', 'attention_mask', 'labels'] | rows: 11712\n",
      "val_ds   → ['input_ids', 'attention_mask', 'labels'] | rows: 1464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation -> HF Datasets\n",
    "TEXT_COL  = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "LABELS   = [\"negative\", \"neutral\", \"positive\"]\n",
    "label2id = {lab: i for i, lab in enumerate(LABELS)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def encode(batch):\n",
    "    enc = tok(batch[TEXT_COL],\n",
    "              truncation=True, padding=\"max_length\", max_length=128)\n",
    "    enc[\"labels\"] = [label2id[x] for x in batch[LABEL_COL]]\n",
    "    return enc\n",
    "\n",
    "cols = [TEXT_COL, LABEL_COL]\n",
    "train_ds = (Dataset.from_pandas(pd.concat([X_train, y_train], axis=1)[cols])\n",
    "                     .map(encode, batched=True, remove_columns=cols))\n",
    "val_ds   = (Dataset.from_pandas(pd.concat([X_val,   y_val],   axis=1)[cols])\n",
    "                     .map(encode, batched=True, remove_columns=cols))\n",
    "\n",
    "print(\"train_ds →\", train_ds.column_names, \"| rows:\", train_ds.num_rows)\n",
    "print(\"val_ds   →\", val_ds.column_names,   \"| rows:\", val_ds.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2b109",
   "metadata": {},
   "source": [
    "# 4. Model Instantiation  \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(LABELS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3250125",
   "metadata": {},
   "source": [
    "# 5. Training Configuration  \n",
    "***\n",
    "\n",
    "| Hyper-param | Value |\n",
    "|-------------|-------|\n",
    "| **Epochs**  | 2 |\n",
    "| **Batch**   | 16 |\n",
    "| **LR**      | 2 × 10⁻⁵ |\n",
    "| **Weight decay** | 0.01 |\n",
    "| **Eval / Save cadence** | once per *epoch* |\n",
    "| **Best-model criterion** | `val_f1` (macro) |\n",
    "\n",
    "`TrainingArguments` keeps only the last **2** checkpoints to save disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68342dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments \n",
    "EPOCHS        = 2\n",
    "BATCH_SIZE    = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir              = MODEL_DIR / \"checkpoints\",\n",
    "    eval_strategy           = \"epoch\",\n",
    "    save_strategy           = \"epoch\",\n",
    "    load_best_model_at_end  = True,\n",
    "    metric_for_best_model   = \"eval_f1\",\n",
    "    greater_is_better       = True,\n",
    "    learning_rate           = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size  = BATCH_SIZE,\n",
    "    num_train_epochs        = EPOCHS,\n",
    "    weight_decay            = 0.01,\n",
    "    seed                    = SEED,\n",
    "    logging_steps           = 50,\n",
    "    save_total_limit        = 2,    \n",
    "    report_to               = \"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859e879",
   "metadata": {},
   "source": [
    "# 6. Trainer & Fine-Tune Results  \n",
    "***\n",
    "\n",
    "| Epoch | Train Loss | Val Loss | Accuracy | F1 (macro) |\n",
    "|-------|-----------:|---------:|---------:|-----------:|\n",
    "| **1** | 0.485 000 | 0.410 365 | 0.837 432 | 0.789 787 |\n",
    "| **2** | 0.319 500 | 0.419 928 | **0.840 164** | **0.798 308** |\n",
    "\n",
    "> **Interpretation**  \n",
    "> • Rapid convergence in two epochs; validation accuracy ~84 %.  \n",
    "> • Macro-F1 ≈ 0.80 suggests balanced performance across classes despite label skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b6697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1464' max='1464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1464/1464 1:15:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.410365</td>\n",
       "      <td>0.837432</td>\n",
       "      <td>0.787987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.419298</td>\n",
       "      <td>0.840164</td>\n",
       "      <td>0.798038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1464, training_loss=0.4235726233388557, metrics={'train_runtime': 4505.8953, 'train_samples_per_second': 5.199, 'train_steps_per_second': 0.325, 'total_flos': 775742920556544.0, 'train_loss': 0.4235726233388557, 'epoch': 2.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer + Fine‑Tune \n",
    "data_collator = DataCollatorWithPadding(tokenizer=tok, return_tensors=\"pt\")\n",
    "\n",
    "metric_acc = load_metric(\"accuracy\")\n",
    "metric_f1  = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds = eval_pred.predictions.argmax(-1)\n",
    "    refs  = eval_pred.label_ids\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=refs)[\"accuracy\"],\n",
    "        \"f1\": metric_f1.compute(predictions=preds, references=refs, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = train_args,\n",
    "    train_dataset   = train_ds,\n",
    "    eval_dataset    = val_ds,\n",
    "    data_collator   = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a40ba7",
   "metadata": {},
   "source": [
    "# 7. Save Artifacts & Export  \n",
    "***\n",
    "*Outputs written to `models/distilbert_twitter/`*\n",
    "\n",
    "- **Model weights & config** `.../final/`  \n",
    "- **Tokenizer vocab & config** `.../tokenizer/`  \n",
    "- **Validation metrics JSON** `val_metrics.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeeb88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [92/92 01:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Artefacts saved to C:\\Projects\\twitter-airline-analysis\\models\\distilbert_twitter\\final\n",
      "{'eval_loss': 0.41929781436920166,\n",
      " 'eval_accuracy': 0.8401639344262295,\n",
      " 'eval_f1': 0.7980384320135547,\n",
      " 'eval_runtime': 63.4491,\n",
      " 'eval_samples_per_second': 23.074,\n",
      " 'eval_steps_per_second': 1.45,\n",
      " 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "VAL_METRICS = trainer.evaluate()            # fetch best‑epoch metrics\n",
    "\n",
    "SAVE_DIR = MODEL_DIR / \"final\"\n",
    "TOKEN_DIR = SAVE_DIR / \"tokenizer\"\n",
    "\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# model & tokenizer\n",
    "trainer.save_model(SAVE_DIR)                # saves both config & weights\n",
    "tok.save_pretrained(TOKEN_DIR)\n",
    "\n",
    "# metrics\n",
    "with open(SAVE_DIR / \"val_metrics.json\", \"w\") as fp:\n",
    "    json.dump(VAL_METRICS, fp, indent=2)\n",
    "\n",
    "print(\"Artefacts saved to\", SAVE_DIR.resolve())\n",
    "pprint.pp(VAL_METRICS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-sentiment-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
