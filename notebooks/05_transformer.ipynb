{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d51277",
   "metadata": {},
   "source": [
    "# 05 – Transformer Fine‑Tune (DistilBERT)\n",
    "\n",
    "Fine‑tune a lightweight transformer (DistilBERT) on the **Twitter‑Airline Sentiment** dataset and benchmark it against a classical **TF‑IDF + Logistic Regression** baseline.\n",
    "\n",
    "> **Model** `distilbert‑base‑uncased`  \n",
    "> **Training split** 90 % of cleaned data (stratified)  \n",
    "> **Validation split** 10 % (held‑out during fine‑tuning)  \n",
    "> **Test set** Untouched split created in `04_baseline_model.ipynb`  \n",
    "> **Artifacts saved to** `models/distilbert_twitter/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84afaf23",
   "metadata": {},
   "source": [
    "## 0 Imports & Global Config\n",
    "\n",
    "Everything we need in one place:\n",
    "\n",
    "1. **Path handling** (`pathlib.Path`) so the notebook is platform‑agnostic.  \n",
    "2. **Reproducibility seeds** for Python, NumPy, and (if available) CUDA.  \n",
    "3. **Key Hugging Face classes** (`AutoTokenizer`, `AutoModelForSequenceClassification`, `Trainer`, …).  \n",
    "4. A line that tells Transformers to **ignore TensorFlow** so only PyTorch is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac48629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% 0 Imports & Global Config ──────────────────────────────────────\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"          # use PyTorch only\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "# reproducibility ---------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# repo‑aware paths --------------------------------------------------\n",
    "PROJ_ROOT = Path.cwd().parent\n",
    "PROC_DIR  = PROJ_ROOT / \"data\" / \"processed\"\n",
    "MODEL_DIR = PROJ_ROOT / \"models\" / \"distilbert_twitter\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfdf44",
   "metadata": {},
   "source": [
    "## 1 Load Pre‑made Feather Splits\n",
    "\n",
    "Read the parquet file that contains **14 640 pre‑cleaned tweets** and show the first few rows to confirm the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6723e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train | rows: 11,712\n",
      "val   | rows: 1,464\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>over an hour on hold so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>your gif game is strong.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm excited too, but perhaps you could scale y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>while other airlines weren't cancelled flighti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conf number fmjtyl delayed - any chance of get...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                        over an hour on hold so far\n",
       "1                           your gif game is strong.\n",
       "2  i'm excited too, but perhaps you could scale y...\n",
       "3  while other airlines weren't cancelled flighti...\n",
       "4  conf number fmjtyl delayed - any chance of get..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    negative\n",
       "1    negative\n",
       "2    positive\n",
       "3    negative\n",
       "4     neutral\n",
       "Name: label, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% 1 Load pre‑made Feather splits ─────────────────────────────────\n",
    "def _load_xy_split(split: str):\n",
    "    \"\"\"\n",
    "    Return (X, y) for the given split.\n",
    "    X : DataFrame with 'text'\n",
    "    y : Series with 'label'\n",
    "    \"\"\"\n",
    "    X = pd.read_feather(PROC_DIR / f\"X_{split}.ftr\")        # ['text']\n",
    "    y = pd.read_feather(PROC_DIR / f\"y_{split}.ftr\")[\"label\"]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = _load_xy_split(\"train\")\n",
    "X_val,   y_val   = _load_xy_split(\"val\")\n",
    "\n",
    "for name, X, y in [(\"train\", X_train, y_train), (\"val\", X_val, y_val)]:\n",
    "    assert list(X.columns) == [\"text\"]\n",
    "    assert y.name == \"label\"\n",
    "    assert len(X) == len(y)\n",
    "    print(f\"{name:5} | rows: {len(X):,}\")\n",
    "\n",
    "display(X_train.head())\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf81ad",
   "metadata": {},
   "source": [
    "## 3 Tokenisation → HF Datasets\n",
    "\n",
    "1. Build a label ↔ ID mapping.  \n",
    "2. Use DistilBERT’s tokenizer to turn each tweet into `input_ids` and `attention_mask`.  \n",
    "3. Convert pandas DataFrames into **`datasets.Dataset`** objects for high‑speed, on‑disk caching.  \n",
    "4. Remove raw text columns so the dataset now holds **tensors only** (`input_ids`, `attention_mask`, `labels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f89e742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11712/11712 [00:01<00:00, 9715.33 examples/s] \n",
      "Map: 100%|██████████| 1464/1464 [00:00<00:00, 3798.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds → ['input_ids', 'attention_mask', 'labels'] | rows: 11712\n",
      "val_ds   → ['input_ids', 'attention_mask', 'labels'] | rows: 1464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% 2 Tokenisation → HF Datasets ──────────────────────────────────\n",
    "TEXT_COL  = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "LABELS   = [\"negative\", \"neutral\", \"positive\"]\n",
    "label2id = {lab: i for i, lab in enumerate(LABELS)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def encode(batch):\n",
    "    enc = tok(batch[TEXT_COL],\n",
    "              truncation=True, padding=\"max_length\", max_length=128)\n",
    "    enc[\"labels\"] = [label2id[x] for x in batch[LABEL_COL]]\n",
    "    return enc\n",
    "\n",
    "cols = [TEXT_COL, LABEL_COL]\n",
    "train_ds = (Dataset.from_pandas(pd.concat([X_train, y_train], axis=1)[cols])\n",
    "                     .map(encode, batched=True, remove_columns=cols))\n",
    "val_ds   = (Dataset.from_pandas(pd.concat([X_val,   y_val],   axis=1)[cols])\n",
    "                     .map(encode, batched=True, remove_columns=cols))\n",
    "\n",
    "print(\"train_ds →\", train_ds.column_names, \"| rows:\", train_ds.num_rows)\n",
    "print(\"val_ds   →\", val_ds.column_names,   \"| rows:\", val_ds.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2b109",
   "metadata": {},
   "source": [
    "## 4 Model Instantiation\n",
    "\n",
    "Load DistilBERT with a **new classification head** sized for 3 labels.  \n",
    "Hugging Face warns that the classification weights are randomly initialised—exactly what we want before fine‑tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b51087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# %% 3 Model Instantiation ─────────────────────────────────────────\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(LABELS),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3250125",
   "metadata": {},
   "source": [
    "## 5 Training Arguments\n",
    "\n",
    "Define *how* we train:\n",
    "\n",
    "* 2 epochs, batch‑size 16, learning‑rate 2 × 10⁻⁵  \n",
    "* Evaluate and save a checkpoint **once per epoch**  \n",
    "* Basic weight‑decay and logging cadence\n",
    "\n",
    "> **Note** Older versions of Transformers expect `eval_strategy`  \n",
    "> whereas ≥ 3.4 use `evaluation_strategy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68342dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 4 Training Arguments ──────────────────────────────────────────\n",
    "EPOCHS        = 2\n",
    "BATCH_SIZE    = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir              = MODEL_DIR / \"checkpoints\",\n",
    "    eval_strategy           = \"epoch\",\n",
    "    save_strategy           = \"epoch\",\n",
    "    load_best_model_at_end  = True,\n",
    "    metric_for_best_model   = \"eval_f1\",\n",
    "    greater_is_better       = True,\n",
    "    learning_rate           = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size  = BATCH_SIZE,\n",
    "    num_train_epochs        = EPOCHS,\n",
    "    weight_decay            = 0.01,\n",
    "    seed                    = SEED,\n",
    "    logging_steps           = 50,\n",
    "    save_total_limit        = 2,      # keep last 2 checkpoints only\n",
    "    report_to               = \"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859e879",
   "metadata": {},
   "source": [
    "## 6 Trainer + Fine‑Tune\n",
    "\n",
    "Glue everything together:\n",
    "\n",
    "1. **DataCollatorWithPadding** dynamically pads each batch.  \n",
    "2. **compute_metrics** returns accuracy and macro‑F1 after every validation pass.  \n",
    "3. **Trainer.train()** runs the full training loop and prints a neat progress bar plus validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "531b6697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1464' max='1464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1464/1464 1:11:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.410365</td>\n",
       "      <td>0.837432</td>\n",
       "      <td>0.787987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.419298</td>\n",
       "      <td>0.840164</td>\n",
       "      <td>0.798038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1464, training_loss=0.4235726233388557, metrics={'train_runtime': 4311.2734, 'train_samples_per_second': 5.433, 'train_steps_per_second': 0.34, 'total_flos': 775742920556544.0, 'train_loss': 0.4235726233388557, 'epoch': 2.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% 5 Trainer + Fine‑Tune ─────────────────────────────────────────\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tok, return_tensors=\"pt\")\n",
    "\n",
    "metric_acc = load_metric(\"accuracy\")\n",
    "metric_f1  = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds = eval_pred.predictions.argmax(-1)\n",
    "    refs  = eval_pred.label_ids\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=refs)[\"accuracy\"],\n",
    "        \"f1\": metric_f1.compute(predictions=preds, references=refs, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = train_args,\n",
    "    train_dataset   = train_ds,\n",
    "    eval_dataset    = val_ds,\n",
    "    data_collator   = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a40ba7",
   "metadata": {},
   "source": [
    "## 8 Save Artifacts & Export\n",
    "\n",
    "Persist everything required for later inference or sharing:\n",
    "\n",
    "* **Fine‑tuned model weights** (`models/distilbert_twitter/final/`)  \n",
    "* **Tokenizer vocab & config** (`models/distilbert_twitter/tokenizer/`)  \n",
    "* **Validation metrics** as a tiny CSV for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeeb88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [92/92 01:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Artefacts saved to C:\\Projects\\twitter-airline-analysis\\models\\distilbert_twitter\\final\n",
      "{'eval_loss': 0.41929781436920166,\n",
      " 'eval_accuracy': 0.8401639344262295,\n",
      " 'eval_f1': 0.7980384320135547,\n",
      " 'eval_runtime': 62.0646,\n",
      " 'eval_samples_per_second': 23.588,\n",
      " 'eval_steps_per_second': 1.482,\n",
      " 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# %% 6 Save Artefacts & Export ─────────────────────────────────────\n",
    "VAL_METRICS = trainer.evaluate()            # fetch best‑epoch metrics\n",
    "\n",
    "SAVE_DIR = MODEL_DIR / \"final\"\n",
    "TOKEN_DIR = SAVE_DIR / \"tokenizer\"\n",
    "\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# model & tokenizer\n",
    "trainer.save_model(SAVE_DIR)                # saves both config & weights\n",
    "tok.save_pretrained(TOKEN_DIR)\n",
    "\n",
    "# metrics\n",
    "with open(SAVE_DIR / \"val_metrics.json\", \"w\") as fp:\n",
    "    json.dump(VAL_METRICS, fp, indent=2)\n",
    "\n",
    "print(\"✅ Artefacts saved to\", SAVE_DIR.resolve())\n",
    "pprint.pp(VAL_METRICS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-sentiment-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
