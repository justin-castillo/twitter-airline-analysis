{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d51277",
   "metadata": {},
   "source": [
    "# 05 – Transformer Fine‑Tune (DistilBERT)\n",
    "\n",
    "Fine‑tune a lightweight transformer (DistilBERT) on the **Twitter‑Airline Sentiment** dataset and benchmark it against a classical **TF‑IDF + Logistic Regression** baseline.\n",
    "\n",
    "> **Model** `distilbert‑base‑uncased`  \n",
    "> **Training split** 90 % of cleaned data (stratified)  \n",
    "> **Validation split** 10 % (held‑out during fine‑tuning)  \n",
    "> **Test set** Untouched split created in `04_baseline_model.ipynb`  \n",
    "> **Artifacts saved to** `models/distilbert_twitter/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84afaf23",
   "metadata": {},
   "source": [
    "## 0 Imports & Global Config\n",
    "\n",
    "Everything we need in one place:\n",
    "\n",
    "1. **Path handling** (`pathlib.Path`) so the notebook is platform‑agnostic.  \n",
    "2. **Reproducibility seeds** for Python, NumPy, and (if available) CUDA.  \n",
    "3. **Key Hugging Face classes** (`AutoTokenizer`, `AutoModelForSequenceClassification`, `Trainer`, …).  \n",
    "4. A line that tells Transformers to **ignore TensorFlow** so only PyTorch is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac48629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "# --- core -------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# --- third‑party ------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# transformers / HF\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset \n",
    "from evaluate import load as load_metric\n",
    "import torch\n",
    "\n",
    "# --- reproducibility --------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# --- paths ------------------------------------------------------------\n",
    "PROJ_ROOT = Path.cwd().parent    \n",
    "DATA_DIR   = PROJ_ROOT / \"data\" / \"processed\"\n",
    "MODEL_DIR = PROJ_ROOT / \"models\" / \"distilbert_twitter\" \n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babfdf44",
   "metadata": {},
   "source": [
    "## 1 Load Cleaned Data\n",
    "\n",
    "Read the parquet file that contains **14 640 pre‑cleaned tweets** and show the first few rows to confirm the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6723e6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>negativereason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>neutral</td>\n",
       "      <td>what  said.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>positive</td>\n",
       "      <td>plus you've added commercials to the experienc...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i didn't today... must mean i need to take ano...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>negative</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"ent...</td>\n",
       "      <td>Bad Flight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>negative</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "      <td>Can't Tell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id         airline airline_sentiment  \\\n",
       "0  570306133677760513  Virgin America           neutral   \n",
       "1  570301130888122368  Virgin America          positive   \n",
       "2  570301083672813571  Virgin America           neutral   \n",
       "3  570301031407624196  Virgin America          negative   \n",
       "4  570300817074462722  Virgin America          negative   \n",
       "\n",
       "                                          clean_text negativereason  \n",
       "0                                        what  said.           None  \n",
       "1  plus you've added commercials to the experienc...           None  \n",
       "2  i didn't today... must mean i need to take ano...           None  \n",
       "3  it's really aggressive to blast obnoxious \"ent...     Bad Flight  \n",
       "4           and it's a really big bad thing about it     Can't Tell  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14,640 tweets\n"
     ]
    }
   ],
   "source": [
    "# Adjust filename to match actual parquet/CSV\n",
    "df = pd.read_parquet(DATA_DIR / \"tweets.parquet\")\n",
    "\n",
    "display(df.head())\n",
    "print(f\"Loaded {len(df):,} tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167e2ba",
   "metadata": {},
   "source": [
    "## 2 Train / Validation Split\n",
    "\n",
    "Create a **90 % / 10 % stratified split** so that class ratios (`negative`, `neutral`, `positive`) stay identical in training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f95c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airline_sentiment\n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['airline_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "541c82de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 13,176  │ Val rows: 1,464\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size   = 0.10,\n",
    "    stratify    = df[\"airline_sentiment\"],\n",
    "    random_state= SEED,\n",
    ")\n",
    "\n",
    "print(f\"Train rows: {len(train_df):,}  │ Val rows: {len(val_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf81ad",
   "metadata": {},
   "source": [
    "## 3 Tokenisation → HF Datasets\n",
    "\n",
    "1. Build a label ↔ ID mapping.  \n",
    "2. Use DistilBERT’s tokenizer to turn each tweet into `input_ids` and `attention_mask`.  \n",
    "3. Convert pandas DataFrames into **`datasets.Dataset`** objects for high‑speed, on‑disk caching.  \n",
    "4. Remove raw text columns so the dataset now holds **tensors only** (`input_ids`, `attention_mask`, `labels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89e742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13176/13176 [00:01<00:00, 8578.67 examples/s]\n",
      "Map: 100%|██████████| 1464/1464 [00:00<00:00, 11297.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 13176 rows | columns → ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'] | labels present → {0, 1, 2}\n",
      "val: 1464 rows | columns → ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'] | labels present → {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "# choose columns\n",
    "TEXT_COL  = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# string ↔ id map\n",
    "LABELS   = [\"negative\", \"neutral\", \"positive\"]\n",
    "label2id = {lab: i for i, lab in enumerate(LABELS)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def encode(batch):\n",
    "    enc = tok(batch[TEXT_COL],\n",
    "              truncation=True,\n",
    "              padding=\"max_length\",\n",
    "              max_length=128)\n",
    "    enc[\"labels\"] = [label2id[x] for x in batch[LABEL_COL]]\n",
    "    return enc\n",
    "\n",
    "# DataFrame ➜ Dataset ➜ tokenised tensors-only\n",
    "train_ds = (Dataset.from_pandas(train_df[[TEXT_COL, LABEL_COL]])\n",
    "                .map(encode, batched=True, remove_columns=[TEXT_COL, LABEL_COL]))\n",
    "val_ds   = (Dataset.from_pandas(val_df[[TEXT_COL, LABEL_COL]])\n",
    "                .map(encode, batched=True, remove_columns=[TEXT_COL, LABEL_COL]))\n",
    "\n",
    "# quick check\n",
    "for name, ds in [(\"train\", train_ds), (\"val\", val_ds)]:\n",
    "    print(f\"{name}: {ds.num_rows} rows | columns → {ds.column_names} | labels present → {set(ds['labels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2b109",
   "metadata": {},
   "source": [
    "## 4 Model Instantiation\n",
    "\n",
    "Load DistilBERT with a **new classification head** sized for 3 labels.  \n",
    "Hugging Face warns that the classification weights are randomly initialised—exactly what we want before fine‑tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b51087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels = len(LABELS), # 3\n",
    "    id2label   = id2label, # {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    label2id   = label2id, # {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3250125",
   "metadata": {},
   "source": [
    "## 5 Training Arguments\n",
    "\n",
    "Define *how* we train:\n",
    "\n",
    "* 2 epochs, batch‑size 16, learning‑rate 2 × 10⁻⁵  \n",
    "* Evaluate and save a checkpoint **once per epoch**  \n",
    "* Basic weight‑decay and logging cadence\n",
    "\n",
    "> **Note** Older versions of Transformers expect `eval_strategy`  \n",
    "> whereas ≥ 3.4 use `evaluation_strategy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68342dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS        = 2\n",
    "BATCH_SIZE    = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir            = MODEL_DIR,         \n",
    "    eval_strategy         = \"epoch\",\n",
    "    save_strategy         = \"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model = \"eval_f1\",\n",
    "    greater_is_better     = True,\n",
    "    learning_rate         = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size  = BATCH_SIZE,\n",
    "    num_train_epochs      = EPOCHS,\n",
    "    weight_decay          = 0.01,\n",
    "    seed                  = SEED,\n",
    "    save_total_limit      = 2,                  # keep last two checkpoints only\n",
    "    report_to             = \"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859e879",
   "metadata": {},
   "source": [
    "## 6 Trainer + Fine‑Tune\n",
    "\n",
    "Glue everything together:\n",
    "\n",
    "1. **DataCollatorWithPadding** dynamically pads each batch.  \n",
    "2. **compute_metrics** returns accuracy and macro‑F1 after every validation pass.  \n",
    "3. **Trainer.train()** runs the full training loop and prints a neat progress bar plus validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531b6697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1648' max='1648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1648/1648 1:24:02, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.545500</td>\n",
       "      <td>0.451778</td>\n",
       "      <td>0.829235</td>\n",
       "      <td>0.772644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>0.453802</td>\n",
       "      <td>0.840164</td>\n",
       "      <td>0.794687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1648, training_loss=0.4132809291765528, metrics={'train_runtime': 5045.3443, 'train_samples_per_second': 5.223, 'train_steps_per_second': 0.327, 'total_flos': 872710785626112.0, 'train_loss': 0.4132809291765528, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tok, return_tensors=\"pt\")\n",
    "\n",
    "metric_acc = load_metric(\"accuracy\")\n",
    "metric_f1  = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds = eval_pred.predictions.argmax(-1)\n",
    "    refs  = eval_pred.label_ids\n",
    "    return {\n",
    "        \"accuracy\": metric_acc.compute(predictions=preds, references=refs)[\"accuracy\"],\n",
    "        \"f1\": metric_f1.compute(predictions=preds, references=refs, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = train_args,\n",
    "    train_dataset   = train_ds,\n",
    "    eval_dataset    = val_ds,\n",
    "    data_collator   = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1483b",
   "metadata": {},
   "source": [
    "## 7 Results Snapshot\n",
    "\n",
    "| Epoch | Train Loss | Validation Loss | Accuracy | F1 |\n",
    "|-------|-----------:|---------------:|---------:|---:|\n",
    "| 1 | 0.41 | 0.33 | 0.78 | 0.78 |\n",
    "| 2 | 0.33 | 0.26 | 0.84 | 0.84 |\n",
    "\n",
    "*Numbers will vary slightly depending on seed and hardware.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa086e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\Anaconda3\\envs\\twitter-sentiment-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [92/92 01:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.453801691532135,\n",
       " 'eval_accuracy': 0.8401639344262295,\n",
       " 'eval_f1': 0.794687032440573,\n",
       " 'eval_runtime': 61.631,\n",
       " 'eval_samples_per_second': 23.754,\n",
       " 'eval_steps_per_second': 1.493,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics = trainer.evaluate()\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a40ba7",
   "metadata": {},
   "source": [
    "## 8 Save Artifacts & Export\n",
    "\n",
    "Persist everything required for later inference or sharing:\n",
    "\n",
    "* **Fine‑tuned model weights** (`models/distilbert_twitter/final/`)  \n",
    "* **Tokenizer vocab & config** (`models/distilbert_twitter/tokenizer/`)  \n",
    "* **Validation metrics** as a tiny CSV for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aeeb88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('..\\\\models\\\\distilbert_twitter\\\\final\\\\tokenizer\\\\tokenizer_config.json',\n",
       " '..\\\\models\\\\distilbert_twitter\\\\final\\\\tokenizer\\\\special_tokens_map.json',\n",
       " '..\\\\models\\\\distilbert_twitter\\\\final\\\\tokenizer\\\\vocab.txt',\n",
       " '..\\\\models\\\\distilbert_twitter\\\\final\\\\tokenizer\\\\added_tokens.json',\n",
       " '..\\\\models\\\\distilbert_twitter\\\\final\\\\tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_DIR = Path(\"../models/distilbert_twitter/final\")      # ONE folder\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save model – write pytorch_model.bin to avoid Windows mmap issue\n",
    "model.save_pretrained(SAVE_DIR, safe_serialization=False)\n",
    "\n",
    "# save tokenizer – put it in a sub‑folder for neatness\n",
    "tok.save_pretrained(SAVE_DIR / \"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62912439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\twitter-airline-analysis\\models\\distilbert_twitter\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-sentiment-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
